{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Sign Recognition Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load The Data\n",
    "\n",
    "**Note: ** I copied some files around to make my readTrafficSigns function work for both training and test sets.\n",
    "\n",
    "Take a look at the test_file / training_file variables as a guide \n",
    "\n",
    "[Training Data](http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip)\n",
    "\n",
    "[Test Data](http://benchmark.ini.rub.de/Dataset/GTSRB_Online-Test-Images-Sorted.zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn import metrics, model_selection\n",
    "\n",
    "\n",
    "training_file = \"./traffic-signs-data/train/Images\"\n",
    "test_file = \"./traffic-signs-data/test/Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTrafficSigns(rootpath):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    # loop over all 42 classes\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        next(gtReader) # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            images.append(plt.imread(prefix + row[0])) # the 1th column is the filename\n",
    "            labels.append(row[7]) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = readTrafficSigns(training_file)\n",
    "x_test, y_test = readTrafficSigns(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image classification integer:  7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABZCAYAAACzIkPrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLI+An1keAT+zPAJ+ZnkE/MzyCPiZ5f8Dk0YP6AW3KGkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1130febe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "index = random.randint(0, len(x_train))\n",
    "image = x_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print('Image classification integer: ', y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tensorflow and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "KEEP_PROB = .80\n",
    "LEARNING_RATE = 0.001\n",
    "COLOR_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment dataset\n",
    "\n",
    "This originally added some skew, but now it just scales the images to 32x32, the original images came in various dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Scale({\"height\":32, \"width\":32})\n",
    "], random_order=True) # apply augmenters in random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n"
     ]
    }
   ],
   "source": [
    "augmented_x_train = np.array(seq.augment_images(x_train))\n",
    "augmented_y_train = y_train\n",
    "x_test = np.array(seq.augment_images(x_test))\n",
    "print(\"Number of training examples =\", augmented_x_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x_train):\n",
    "    x_train = np.array(x_train)\n",
    "    x_train = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rgb_to_greyscale(image):\n",
    "    return np.dot(image[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def process_data(x_data, y_data):\n",
    "    if COLOR_CHANNELS == 1:\n",
    "        x_data = rgb_to_greyscale(x_data)\n",
    "        x_data = x_data.reshape(x_data.shape + (1,))\n",
    "    x_data = normalize(x_data)\n",
    "    x_data, y_data = shuffle(x_data, y_data)\n",
    "    return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = process_data(augmented_x_train, augmented_y_train)\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train, y_train, test_size=0.2)\n",
    "x_test, y_test = process_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = len(set(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def traffic_signs(x):    \n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, COLOR_CHANNELS, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(1600, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    fc1    = tf.nn.dropout(fc1, KEEP_PROB)\n",
    "\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(n_classes))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, COLOR_CHANNELS))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "pred = traffic_signs(x)\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "\n",
    "logits = traffic_signs(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(x_data, y_data):\n",
    "    num_examples = len(x_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = x_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.545\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.760\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.861\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.882\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.913\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.924\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.935\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.945\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.948\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.946\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(x_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        x_train, y_train = shuffle(x_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = np.array(x_train[offset:end]), np.array(y_train[offset:end])\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(x_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './traffic_lights')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
